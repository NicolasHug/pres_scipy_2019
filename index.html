

<!DOCTYPE html>
<html>
  <head>
    <title>Gradient Boosting</title>
    <meta charset="utf-8">
        <style>
       @import url(https://fonts.googleapis.com/css?family=Garamond);
       @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
       @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
     </style>
     <link rel="stylesheet" href="../style.css">
  </head>
  <body>
    <textarea id="source">

class:center, middle

# Fast Gradient Boosting with Numba

---

class:middle

Nicolas Hug, Columbia University -- [@hug_nicolas](https://twitter.com/hug_nicolas/)

Olivier Grisel, INRIA Paris -- [@ogrisel](https://twitter.com/ogrisel/)

[pygbm](https://github.com/ogrisel/pygbm):
fast gradient boosting in Python with `Numba`

### Outline

1. Gradient Boosting is a gradient descent
2. Making it fast with histograms
3. `Numba`: field report (and a tiny bit of `Cython`)

&nbsp;
&nbsp;
&nbsp;

.center[slides at [https://bit.ly/2YFaIPO](https://bit.ly/2YFaIPO)]


---

class: middle
# Gradient Boosting

`$$\hat{y}_i = \sum_{m = 1}^{\text{n_iter}} h_m(\mathbf{x}_i)$$`

$h_m$: "weak" estimator that predicts **gradients** (not the target)

This is a gradient descent!

---

.center[![:scale 100%](gradient_boosting_1.jpg)]

---

.center[![:scale 100%](gradient_boosting_2.jpg)]

---

.center[![:scale 100%](gradient_boosting_3.jpg)]

---

#### Hack # 1

Optimize w.r.t. predictions: $~~~~\hat{\mathbf{y}}^{(m + 1)} = \hat{\mathbf{y}}^{(m)} - \alpha
   \frac{\partial \mathcal{L}}{\partial \hat{\mathbf{y}}^{(m)}}$

```py
def compute_gradient(y_true, y_pred):
    return -2 * (y_true - y_pred)  # Least squares loss gradient

y_pred = np.zeros(shape=n_samples)

for m in range(n_iterations):
    negative_gradient = -compute_gradient(y_true, y_pred)
    y_pred += alpha * negative_gradient

    # save training loss value for plotting
    loss_values.append(np.mean((y_true - y_pred)**2))
```
.center[![:scale 50%](loss_vs_iter.png)]

--

.center[Prediction for new data???]

---

#### Hack # 2

Train weak predictor $h$ to predict the gradients!


```py
for m in range(n_iterations):
    negative_gradient = -compute_gradient(y_true, y_pred)
    h = DecisionTree().fit(X, y=negative_gradient)
    y_pred += alpha * h.predict(X)
    self.predictors.append(h)  # save predictor for later
```

We can now use the $h_m$ for new data

```py
def predict(X):
    pred = np.zeros(...)
    for h in self.predictors:
        pred += alpha * h.predict(X)
    return pred
```

`$$\hat{y}_i = \sum_{m = 1}^{\text{n_iter}} h_m(\mathbf{x}_i)$$`

That's our gradient descent!

---

### Recipe for Gradient Boosting

1. Choose a (differentiable) loss
2. Optimize loss w.r.t **predictions** instead of a model parameter
3. Predict the GD step with a base estimator (a regressor)
4. Use these base estimators to predict unseen data


---

class: center, middle

# Making it fast

Taking inspiration from [LightGBM](https://github.com/microsoft/LightGBM)

---

### What is slow: split finding

```py
for m in range(n_iterations):
    ...
    h = DecisionTree().fit(X, y=negative_gradient)
    ...
```

.center[![:scale 60%](tree_building.jpg)]

.center[Repeated for each tree, at every node]


---

### What is slow: split finding

.center[![:scale 90%](splitting_1.jpg)]

---

### What is slow: split finding

.center[![:scale 90%](splitting_2.jpg)]

---

### What is slow: split finding

.center[![:scale 90%](splitting_3.jpg)]

--

.center[From $\mathcal{O}(n \log n)$ to $\mathcal{O} (n)$: Histograms]

---

### Bin the data!

.center[![:scale 75%](binning_1.jpg)]

We bin once and for all at the start of the fitting process

---

### Bin the data!

.center[![:scale 75%](binning_2.jpg)]

We bin once and for all at the start of the fitting process

---

### Bin the data!

.center[![:scale 75%](binning_3.jpg)]

We bin once and for all at the start of the fitting process

Histograms are then computed at each node, for each feature

---

### Split finding now

.center[![:scale 100%](splitting_new_1.jpg)]

---

### Split finding now

.center[![:scale 100%](splitting_new_2.jpg)]

---

### Split finding now

.center[![:scale 100%](splitting_new_3.jpg)]

---

### Subtraction trick

.center[![:scale 55%](hist_sub.jpg)]


.center[Samples are mapped either to left or right child]

.center[Hist(parent) = Hist(left) + Hist(right)]

--

.center[For half of the nodes:]
.center[$\mathcal{O}(n)$ --> $\mathcal{O}(\text{n_bins})$]


---

class:center, middle

![:scale 100%](numba.png)

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

GBDT implementation with `Numba`:

[github.com/ogrisel/pygbm](https://github.com/ogrisel/pygbm)


---

Support for custom dtypes (structured arrays)

```Python
HISTOGRAM_DTYPE = np.dtype([
    ('g', np.float32),  # sum gradients
    ('h', np.float32),  # sum hessians
])

@njit
def subtract_histograms(hist_a, hist_b):
    out = np.zeros(n_bins, dtype=HISTOGRAM_DTYPE)

    for i in range(n_bins):
        out[i]['g'] = hist_a[i]['g'] - hist_b[i]['g']
        out[i]['h'] = hist_a[i]['h'] - hist_b[i]['h']

    return out
```

--

Support for parallelization (OpenMP or Intel TBB)

```py
for feature in prange(n_features):  # parallel loop
    subtract_histograms(...)
```

--

`Cython` equivalent:
- need to declare C-struct equivalent to DTYPE
- cannot allocate numpy array in function
  - parallel code needs GIL to be released
  - allocate outside and pass it as a parameter

---

Lists of different length? No problem

```py
for m in range(n_iterations):  # main GB loop
    ...
    y_pred += h.predict(X)  # can be optimized for training data!
    ...
```

.center[![:scale 50%](leaves_data.jpg)]

```py
@njit(parallel=True)
def update_predictions(leaves_data, y_pred):
    for leaf in prange(len(leaves_data)):
        leaf_value, sample_indices = leaves_data[leaf]
        for i in sample_indices:
            y_pred[i] += leaf_value
```

---

### It's fast

&nbsp;
&nbsp;
&nbsp;

Higgs boson dataset (10E7 samples), 20 trees, 4 cores

```
pygbm (ignoring compile time!)
done in 42.851s, ROC AUC: 0.7229, ACC: 0.7245
```

```
Lightgbm
done in 42.400s, ROC AUC: 0.7232, ACC: 0.7250
```

---

### Overall experience with Numba

- the doc is good ♥♥♥
- low barrier to entry
- from slow code to fast code: easy process
- code looks like Python (with for loops): stay Pythonic
- it works

--

Things I wish would improve:

- error messages

--
- compilation caching (takes 10s for `pygbm`)

--
- a few bugs still

--
- missing features
    - use parallelization in methods
    - parallel loop over tuples
    - control OpenMP thread scheduling
    - profiling?
    - ...

--

####It's still WIP though

####Use it!


---
class: center, middle

### Thanks!

(BTW, check out our new GBDTs in
[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)!)

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        // highlightStyle: 'magula',
        // highlightSpans: true,
        // highlightLines: true,
        // ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>

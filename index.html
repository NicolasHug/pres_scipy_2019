

<!DOCTYPE html>
<html>
  <head>
    <title>Gradient Boosting</title>
    <meta charset="utf-8">
        <style>
       @import url(https://fonts.googleapis.com/css?family=Garamond);
       @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
       @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
     </style>
     <link rel="stylesheet" href="../style.css">
  </head>
  <body>
    <textarea id="source">

class:center, middle

# Gradient Boosting with Numba

---

class:middle

[Nicolas Hug](http://nicolas-hug.com/), Columbia University

[Olivier Grisel](http://ogrisel.com/), INRIA Paris

[pygbm](https://github.com/ogrisel/pygbm):
fast gradient boosting in Python with Numba

### Outline

1. Gradient Boosting is a gradient descent
2. Making it fast with histograms
3. Pygbm and Numba (and a tiny bit of Cython)


---

# Gradient Boosting

`$$\hat{y}_i = \sum_{m = 1}^{\text{n_iter}} h_m(\mathbf{x}_i)$$`

This is a gradient descent!

$h_m$: "weak" estimator that predicts **gradients** (not the target!)

--

----

Very analoguous to GD for parameter estimation (e.g. linear regression):

- Linear regression: optimize loss w.r.t a parameter

- Gradient boosting: optimize loss w.r.t. "the predictions"


---

#### Linear regression

Loss: $\mathcal{L} = \sum_i (y_i - \hat{y}_i)^2$ where $\hat{y}_i = \mathbf{x}_i^T\mathbf{\theta}$

Derive loss w.r.t. $\mathbf{\theta}$

$$\mathbf{\theta}^{(m + 1)} = \mathbf{\theta}^{(m)} - \text{learning_rate} *
\frac{\partial \mathcal{L}}{\partial \mathbf{\theta}^{(m)}}$$

--

----

#### Gradient Boosting

Loss: $\mathcal{L} = \sum_i (y_i - \hat{y}_i)^2\ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ \hat{\mathbf{y}} =
(\hat{y}_1, ..., \hat{y}_n)$

The loss is a function of $\hat{\mathbf{y}} \in \mathcal{R}^n$ too! (Just like it is a
function of $\mathbf{\theta} \in \mathcal{R}^d$)


Derive loss w.r.t. **the predictions** $\hat{\mathbf{y}}$

$$\hat{\mathbf{y}}^{(m + 1)} = \hat{\mathbf{y}}^{(m)} - \text{learning_rate} *
   \frac{\partial \mathcal{L}}{\partial \hat{\mathbf{y}}^{(m)}}$$

---

$$\hat{\mathbf{y}}^{(m + 1)} = \hat{\mathbf{y}}^{(m)} - \text{learning_rate} *
   \frac{\partial \mathcal{L}}{\partial \hat{\mathbf{y}}^{(m)}}$$

```py
def compute_gradient(y_true, y_pred):
    return -2 * (y_true - y_pred)

y_pred = np.zeros(shape=n_samples)  # (or some better init)
for m in range(n_iterations):
    negative_gradient = -compute_gradient(y_true, y_pred)
    y_pred += learning_rate * negative_gradient

    # save loss value for plotting (least squares loss)
    loss_values.append(np.mean((y_true_train - y_pred_train)**2))
```
.center[![:scale 80%](loss_vs_iter.png)]

---

```py
for m in range(n_iterations):
    negative_gradient = -compute_gradient(y_true, y_pred)
    y_pred += learning_rate * negative_gradient
```

OK but how do I compute predictions for unseen samples?

Let a weak predictor $h$ try to predict the gradients!

```py
for m in range(n_iterations):
    negative_gradient = -compute_gradient(y_true, y_pred)
    h = DecisionTree().fit(X, y=negative_gradient)
    y_pred += learning_rate * h.predict(X)
    self.predictors.append(h)  # save predictor for later
```

```py
def predict(X):
    pred = np.zeros(...)
    for h in self.predictors:
        pred += h.predict(X)
    return pred
```

`$$\hat{y}_i = \sum_{m = 1}^{\text{n_iter}} h_m(\mathbf{x}_i)$$`

That's our gradient descent!

---

### Receipe for Gradient Boosting

1. Optimize loss w.r.t **predictions** instead of a model parameter
2. Predict the gradient step with a base estimator (a regressor)
3. Use these base estimators to predict for unseen data

```py
for m in range(n_iterations):
    negative_gradient = -compute_gradient(y_true, y_pred)
    h = DecisionTree().fit(X, y=negative_gradient)
    y_pred += learning_rate * h.predict(X)
    self.predictors.append(h)  # save predictor for later
```

```py
def predict(X):
    pred = np.zeros(...)
    for h in self.predictors:
        pred += h.predict(X)
    return pred
```

---

class: center, middle

# Making it fast

Taking inspiration from [LightGBM](https://github.com/microsoft/LightGBM)

---

### What is slow: split finding

```py
for m in range(n_iterations):
    ...
    h = DecisionTree().fit(X, y=negative_gradient)
    ...
```

.center[![:scale 60%](tree_building.jpg)]


---

### What is slow: split finding

.center[![:scale 90%](splitting_1.jpg)]

---

### What is slow: split finding

.center[![:scale 90%](splitting_2.jpg)]

---

### What is slow: split finding

.center[![:scale 90%](splitting_3.jpg)]

--

.center[From $\mathcal{O}(n \log n)$ to $\mathcal{O} (n)$: Histograms]

---

### Bin the data!

.center[![:scale 75%](binning_1.jpg)]

We bin once and for all at the start of the fitting process

---

### Bin the data!

.center[![:scale 75%](binning_2.jpg)]

We bin once and for all at the start of the fitting process

---

### Bin the data!

.center[![:scale 75%](binning_3.jpg)]

We bin once and for all at the start of the fitting process

Histograms are then computed at each node, for each feature

---

#### Before / Now

.center[![:scale 85%](new_splitting_1.jpg)]

---

#### Before / Now

.center[![:scale 85%](new_splitting_2.jpg)]

---

#### Before / Now

.center[![:scale 85%](new_splitting_3.jpg)]

---

#### Before / Now

.center[![:scale 85%](new_splitting_4.jpg)]

---

#### Before / Now

.center[![:scale 85%](new_splitting_5.jpg)]


---

### Subtraction trick

.center[![:scale 55%](hist_sub.jpg)]


.center[Samples are mapped either to left or right child]

.center[Hist(parent) = Hist(left) + Hist(right)]

--

.center[Use subtraction trick!]
.center[c = a - b]

.center[$\mathcal{O}(n)$ --> $\mathcal{O}(\text{n_bins})$]


---

class:center, middle

![:scale 100%](numba.png)

&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;
&nbsp;

GBDT implementation with numba:

[github.com/ogrisel/pygbm](https://github.com/ogrisel/pygbm)


---

Support for custom dtypes (structured arrays)

```Python
HISTOGRAM_DTYPE = np.dtype([
    ('g', np.float32),  # sum gradients
    ('h', np.float32),  # sum hessians
])

@njit
def subtract_histograms(hist_a, hist_b):
    out = np.zeros(n_bins, dtype=HISTOGRAM_DTYPE)

    for i in range(n_bins):
        out[i]['g'] = hist_a[i]['g'] - hist_b[i]['g']
        out[i]['h'] = hist_a[i]['h'] - hist_b[i]['h']

    return out
```

--

Support for parallelization (OpenMP)

```py
for feature in prange(n_features):  # parallel loop
    subtract_histograms(...)
```

--

`Cython` equivalent:
- need to declare C-struct equivalent to DTYPE
- cannot allocate numpy array in function
  - parallel code needs GIL to be released
  - allocate outside and pass it as a parameter

---

can optimize list of lists (with different sizes!)

```py
@njit(parallel=True)
def _update_raw_predictions(leaves_data, raw_predictions):
    for leaf_idx in prange(len(leaves_data)):
        leaf_value, sample_indices = leaves_data[leaf_idx]
        for sample_idx in sample_indices:
            raw_predictions[sample_idx] += leaf_value
```

TODO: Ouuuups c'est deprecated, en attente de leur reponse...
---

### Overall experience with Numba

- low barrier to entry
- the doc is good ♥♥♥
- from slow code to fast code: easy process
- code looks like Python (with for loops): stay Pythonic

--

Things I wish would improve:

- error messages

--
- compilation caching (takes 10s for `pygbm`)

--
- a few bugs still

--
- missing features
    - use parallelization in methods
    - parallel loop over tuples
    - control OpenMP thread scheduling
    - ...

--

####It's still WIP though

####Use it!


---
class: center, middle

### Thanks!

(BTW, check out our new GBDTs in
[scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier)!)

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        // highlightStyle: 'magula',
        // highlightSpans: true,
        // highlightLines: true,
        // ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
